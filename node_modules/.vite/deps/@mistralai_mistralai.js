import {
  __publicField
} from "./chunk-WXXH56N5.js";

// node_modules/@mistralai/mistralai/src/client.js
var isNode = false;
var VERSION = "0.0.3";
var RETRY_STATUS_CODES = [429, 500, 502, 503, 504];
var ENDPOINT = "https://api.mistral.ai";
async function initializeFetch() {
  if (typeof window === "undefined" || typeof globalThis.fetch === "undefined") {
    const nodeFetch = await import("./node-fetch.js");
    fetch = nodeFetch.default;
    isNode = true;
  } else {
    fetch = globalThis.fetch;
  }
}
initializeFetch();
var MistralAPIError = class extends Error {
  /**
   * A simple error class for Mistral API errors
   * @param {*} message
   */
  constructor(message) {
    super(message);
    this.name = "MistralAPIError";
  }
};
var MistralClient = class {
  /**
   * A simple and lightweight client for the Mistral API
   * @param {*} apiKey can be set as an environment variable MISTRAL_API_KEY,
   * or provided in this parameter
   * @param {*} endpoint defaults to https://api.mistral.ai
   * @param {*} maxRetries defaults to 5
   * @param {*} timeout defaults to 120 seconds
   */
  constructor(apiKey = process.env.MISTRAL_API_KEY, endpoint = ENDPOINT, maxRetries = 5, timeout = 120) {
    /**
     *
     * @param {*} method
     * @param {*} path
     * @param {*} request
     * @return {Promise<*>}
     */
    __publicField(this, "_request", async function(method, path, request) {
      const url = `${this.endpoint}/${path}`;
      const options = {
        method,
        headers: {
          "User-Agent": `mistral-client-js/${VERSION}`,
          "Accept": (request == null ? void 0 : request.stream) ? "text/event-stream" : "application/json",
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.apiKey}`
        },
        body: method !== "get" ? JSON.stringify(request) : null,
        timeout: this.timeout * 1e3
      };
      for (let attempts = 0; attempts < this.maxRetries; attempts++) {
        try {
          const response = await fetch(url, options);
          if (response.ok) {
            if (request == null ? void 0 : request.stream) {
              if (isNode) {
                return response.body;
              } else {
                const reader = response.body.getReader();
                const asyncIterator = async function* () {
                  try {
                    while (true) {
                      const { done, value } = await reader.read();
                      if (done)
                        return;
                      yield value;
                    }
                  } finally {
                    reader.releaseLock();
                  }
                };
                return asyncIterator();
              }
            }
            return await response.json();
          } else if (RETRY_STATUS_CODES.includes(response.status)) {
            console.debug(
              `Retrying request on response status: ${response.status}`,
              `Response: ${await response.text()}`,
              `Attempt: ${attempts + 1}`
            );
            await new Promise(
              (resolve) => setTimeout(resolve, Math.pow(2, attempts + 1) * 500)
            );
          } else {
            throw new MistralAPIError(
              `HTTP error! status: ${response.status} Response: 
${await response.text()}`
            );
          }
        } catch (error) {
          console.error(`Request failed: ${error.message}`);
          if (error.name === "MistralAPIError") {
            throw error;
          }
          if (attempts === this.maxRetries - 1)
            throw error;
          await new Promise(
            (resolve) => setTimeout(resolve, Math.pow(2, attempts + 1) * 500)
          );
        }
      }
      throw new Error("Max retries reached");
    });
    /**
     * Creates a chat completion request
     * @param {*} model
     * @param {*} messages
     * @param {*} temperature
     * @param {*} maxTokens
     * @param {*} topP
     * @param {*} randomSeed
     * @param {*} stream
     * @param {*} safeMode deprecated use safePrompt instead
     * @param {*} safePrompt
     * @return {Promise<Object>}
     */
    __publicField(this, "_makeChatCompletionRequest", function(model, messages, temperature, maxTokens, topP, randomSeed, stream, safeMode, safePrompt) {
      return {
        model,
        messages,
        temperature: temperature ?? void 0,
        max_tokens: maxTokens ?? void 0,
        top_p: topP ?? void 0,
        random_seed: randomSeed ?? void 0,
        stream: stream ?? void 0,
        safe_prompt: (safeMode || safePrompt) ?? void 0
      };
    });
    /**
     * Returns a list of the available models
     * @return {Promise<Object>}
     */
    __publicField(this, "listModels", async function() {
      const response = await this._request("get", "v1/models");
      return response;
    });
    /**
     * A chat endpoint without streaming
     * @param {*} model the name of the model to chat with, e.g. mistral-tiny
     * @param {*} messages an array of messages to chat with, e.g.
     * [{role: 'user', content: 'What is the best French cheese?'}]
     * @param {*} temperature the temperature to use for sampling, e.g. 0.5
     * @param {*} maxTokens the maximum number of tokens to generate, e.g. 100
     * @param {*} topP the cumulative probability of tokens to generate, e.g. 0.9
     * @param {*} randomSeed the random seed to use for sampling, e.g. 42
     * @param {*} safeMode deprecated use safePrompt instead
     * @param {*} safePrompt whether to use safe mode, e.g. true
     * @return {Promise<Object>}
     */
    __publicField(this, "chat", async function({
      model,
      messages,
      temperature,
      maxTokens,
      topP,
      randomSeed,
      safeMode,
      safePrompt
    }) {
      const request = this._makeChatCompletionRequest(
        model,
        messages,
        temperature,
        maxTokens,
        topP,
        randomSeed,
        false,
        safeMode,
        safePrompt
      );
      const response = await this._request(
        "post",
        "v1/chat/completions",
        request
      );
      return response;
    });
    /**
     * A chat endpoint that streams responses.
     * @param {*} model the name of the model to chat with, e.g. mistral-tiny
     * @param {*} messages an array of messages to chat with, e.g.
     * [{role: 'user', content: 'What is the best French cheese?'}]
     * @param {*} temperature the temperature to use for sampling, e.g. 0.5
     * @param {*} maxTokens the maximum number of tokens to generate, e.g. 100
     * @param {*} topP the cumulative probability of tokens to generate, e.g. 0.9
     * @param {*} randomSeed the random seed to use for sampling, e.g. 42
     * @param {*} safeMode deprecated use safePrompt instead
     * @param {*} safePrompt whether to use safe mode, e.g. true
     * @return {Promise<Object>}
     */
    __publicField(this, "chatStream", async function* ({
      model,
      messages,
      temperature,
      maxTokens,
      topP,
      randomSeed,
      safeMode,
      safePrompt
    }) {
      const request = this._makeChatCompletionRequest(
        model,
        messages,
        temperature,
        maxTokens,
        topP,
        randomSeed,
        true,
        safeMode,
        safePrompt
      );
      const response = await this._request(
        "post",
        "v1/chat/completions",
        request
      );
      let buffer = "";
      const decoder = new TextDecoder();
      for await (const chunk of response) {
        buffer += decoder.decode(chunk, { stream: true });
        let firstNewline;
        while ((firstNewline = buffer.indexOf("\n")) !== -1) {
          const chunkLine = buffer.substring(0, firstNewline);
          buffer = buffer.substring(firstNewline + 1);
          if (chunkLine.startsWith("data:")) {
            const json = chunkLine.substring(6).trim();
            if (json !== "[DONE]") {
              yield JSON.parse(json);
            }
          }
        }
      }
    });
    /**
     * An embeddings endpoint that returns embeddings for a single,
     * or batch of inputs
     * @param {*} model The embedding model to use, e.g. mistral-embed
     * @param {*} input The input to embed,
     * e.g. ['What is the best French cheese?']
     * @return {Promise<Object>}
     */
    __publicField(this, "embeddings", async function({ model, input }) {
      const request = {
        model,
        input
      };
      const response = await this._request("post", "v1/embeddings", request);
      return response;
    });
    this.endpoint = endpoint;
    this.apiKey = apiKey;
    this.maxRetries = maxRetries;
    this.timeout = timeout;
  }
};
var client_default = MistralClient;
export {
  client_default as default
};
//# sourceMappingURL=@mistralai_mistralai.js.map
